{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Ed84042MuH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib import colors as mcolors\n",
        "from scipy.stats import linregress\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=True)"
      ],
      "metadata": {
        "id": "UgSxtKWr321e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Seaborn plot styles: Set background color and use dark grid\n",
        "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')"
      ],
      "metadata": {
        "id": "KMMvOxw94WFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data.csv', encoding=\"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "8KvQJvaq5hF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "lS6blYgQ5lC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Mp_V1eNb5qMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T\n"
      ],
      "metadata": {
        "id": "D-FEWOU65t6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object').T"
      ],
      "metadata": {
        "id": "s2pvt4Lr5wjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (missing_data[missing_data > 0] / df.shape[0]) * 100\n",
        "\n",
        "# Prepare values\n",
        "missing_percentage.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "# Plot the barh chart\n",
        "fig, ax = plt.subplots(figsize=(15, 4))\n",
        "ax.barh(missing_percentage.index, missing_percentage, color='#ff6200')\n",
        "\n",
        "# Annotate the values and indexes\n",
        "for i, (value, name) in enumerate(zip(missing_percentage, missing_percentage.index)):\n",
        "    ax.text(value+0.5, i, f\"{value:.2f}%\", ha='left', va='center', fontweight='bold', fontsize=18, color='black')\n",
        "\n",
        "# Set x-axis limit\n",
        "ax.set_xlim([0, 40])\n",
        "\n",
        "# Add title and xlabel\n",
        "plt.title(\"Percentage of Missing Values\", fontweight='bold', fontsize=22)\n",
        "plt.xlabel('Percentages (%)', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C4xjWsjx52Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['CustomerID'].isnull() | df['Description'].isnull()].head()"
      ],
      "metadata": {
        "id": "yFWMZi-x55gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing rows with missing values in 'CustomerID' and 'Description' columns\n",
        "df = df.dropna(subset=['CustomerID', 'Description'])"
      ],
      "metadata": {
        "id": "06IQDFst59bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the removal of missing values\n",
        "df.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "hS8Df1KV6BRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicate rows (keeping all instances)\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Sorting the data by certain columns to see the duplicate rows next to each other\n",
        "duplicate_rows_sorted = duplicate_rows.sort_values(by=['InvoiceNo', 'StockCode', 'Description', 'CustomerID', 'Quantity'])\n",
        "\n",
        "# Displaying the first 10 records\n",
        "duplicate_rows_sorted.head(10)"
      ],
      "metadata": {
        "id": "v7KHo8Rb6CA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the number of duplicate rows\n",
        "print(f\"The dataset contains {df.duplicated().sum()} duplicate rows that need to be removed.\")\n",
        "\n",
        "# Removing duplicate rows\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "pvP2HoDl6FL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the number of rows in the dataframe\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "bP_wjJwM6LnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out the rows with InvoiceNo starting with \"C\" and create a new column indicating the transaction status\n",
        "df['Transaction_Status'] = np.where(df['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')\n",
        "\n",
        "# Analyze the characteristics of these rows (considering the new column)\n",
        "cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\n",
        "cancelled_transactions.describe().drop('CustomerID', axis=1)"
      ],
      "metadata": {
        "id": "BJny298P6PN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the percentage of cancelled transactions\n",
        "cancelled_percentage = (cancelled_transactions.shape[0] / df.shape[0]) * 100\n",
        "\n",
        "# Printing the percentage of cancelled transactions\n",
        "print(f\"The percentage of cancelled transactions in the dataset is: {cancelled_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "d6W26tw26Qk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the number of unique stock codes\n",
        "unique_stock_codes = df['StockCode'].nunique()\n",
        "\n",
        "# Printing the number of unique stock codes\n",
        "print(f\"The number of unique stock codes in the dataset is: {unique_stock_codes}\")\n"
      ],
      "metadata": {
        "id": "evRmnOZC6UBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the top 10 most frequent stock codes\n",
        "top_10_stock_codes = df['StockCode'].value_counts(normalize=True).head(10) * 100\n",
        "\n",
        "# Plotting the top 10 most frequent stock codes\n",
        "plt.figure(figsize=(12, 5))\n",
        "top_10_stock_codes.plot(kind='barh', color='#ff6200')\n",
        "\n",
        "# Adding the percentage frequency on the bars\n",
        "for index, value in enumerate(top_10_stock_codes):\n",
        "    plt.text(value, index+0.25, f'{value:.2f}%', fontsize=10)\n",
        "\n",
        "plt.title('Top 10 Most Frequent Stock Codes')\n",
        "plt.xlabel('Percentage Frequency (%)')\n",
        "plt.ylabel('Stock Codes')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rjcwVUmP6Xzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the number of numeric characters in each unique stock code\n",
        "unique_stock_codes = df['StockCode'].unique()\n",
        "numeric_char_counts_in_unique_codes = pd.Series(unique_stock_codes).apply(lambda x: sum(c.isdigit() for c in str(x))).value_counts()\n",
        "\n",
        "# Printing the value counts for unique stock codes\n",
        "print(\"Value counts of numeric character frequencies in unique stock codes:\")\n",
        "print(\"-\"*70)\n",
        "print(numeric_char_counts_in_unique_codes)"
      ],
      "metadata": {
        "id": "UnCsHQuB6bKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalous_stock_codes = [code for code in unique_stock_codes if sum(c.isdigit() for c in str(code)) in (0, 1)]\n",
        "\n",
        "# Printing each stock code on a new line\n",
        "print(\"Anomalous stock codes:\")\n",
        "print(\"-\"*22)\n",
        "for code in anomalous_stock_codes:\n",
        "    print(code)"
      ],
      "metadata": {
        "id": "3kVL9ryE6fc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the percentage of records with these stock codes\n",
        "percentage_anomalous = (df['StockCode'].isin(anomalous_stock_codes).sum() / len(df)) * 100\n",
        "\n",
        "# Printing the percentage\n",
        "print(f\"The percentage of records with anomalous stock codes in the dataset is: {percentage_anomalous:.2f}%\")"
      ],
      "metadata": {
        "id": "m3aMVDc36ir9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing rows with anomalous stock codes from the dataset\n",
        "df = df[~df['StockCode'].isin(anomalous_stock_codes)]"
      ],
      "metadata": {
        "id": "f2z6UHD06l8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the number of rows in the dataframe\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "yU1-Hpgd6pUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the occurrence of each unique description and sort them\n",
        "description_counts = df['Description'].value_counts()\n",
        "\n",
        "# Get the top 30 descriptions\n",
        "top_30_descriptions = description_counts[:30]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.barh(top_30_descriptions.index[::-1], top_30_descriptions.values[::-1], color='#ff6200')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Number of Occurrences')\n",
        "plt.ylabel('Description')\n",
        "plt.title('Top 30 Most Frequent Descriptions')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xu56f89S6u5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find unique descriptions containing lowercase characters\n",
        "lowercase_descriptions = df['Description'].unique()\n",
        "lowercase_descriptions = [desc for desc in lowercase_descriptions if any(char.islower() for char in desc)]\n",
        "\n",
        "# Print the unique descriptions containing lowercase characters\n",
        "print(\"The unique descriptions containing lowercase characters are:\")\n",
        "print(\"-\"*60)\n",
        "for desc in lowercase_descriptions:\n",
        "    print(desc)"
      ],
      "metadata": {
        "id": "UaTMBE_H7D6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_related_descriptions = [\"Next Day Carriage\", \"High Resolution Image\"]\n",
        "\n",
        "# Calculate the percentage of records with service-related descriptions\n",
        "service_related_percentage = df[df['Description'].isin(service_related_descriptions)].shape[0] / df.shape[0] * 100\n",
        "\n",
        "# Print the percentage of records with service-related descriptions\n",
        "print(f\"The percentage of records with service-related descriptions in the dataset is: {service_related_percentage:.2f}%\")\n",
        "\n",
        "# Remove rows with service-related information in the description\n",
        "df = df[~df['Description'].isin(service_related_descriptions)]\n",
        "\n",
        "# Standardize the text to uppercase to maintain uniformity across the dataset\n",
        "df['Description'] = df['Description'].str.upper()\n"
      ],
      "metadata": {
        "id": "gm1u-SrH7HeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the number of rows in the dataframe\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "rbkUfdkD7NHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['UnitPrice'].describe()"
      ],
      "metadata": {
        "id": "X8AH5dr47PT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['UnitPrice']==0].describe()[['Quantity']]"
      ],
      "metadata": {
        "id": "0sV9fLmt7SEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing records with a unit price of zero to avoid potential data entry errors\n",
        "df = df[df['UnitPrice'] > 0]"
      ],
      "metadata": {
        "id": "RsdykDn-7UtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the index of the cleaned dataset\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "C8kM_6wk7Xcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the number of rows in the dataframe\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "lAsQia457aQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert InvoiceDate to datetime type\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Convert InvoiceDate to datetime and extract only the date\n",
        "df['InvoiceDay'] = df['InvoiceDate'].dt.date\n",
        "\n",
        "# Find the most recent purchase date for each customer\n",
        "customer_data = df.groupby('CustomerID')['InvoiceDay'].max().reset_index()\n",
        "\n",
        "# Find the most recent date in the entire dataset\n",
        "most_recent_date = df['InvoiceDay'].max()\n",
        "\n",
        "# Convert InvoiceDay to datetime type before subtraction\n",
        "customer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])\n",
        "most_recent_date = pd.to_datetime(most_recent_date)\n",
        "\n",
        "# Calculate the number of days since the last purchase for each customer\n",
        "customer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDay']).dt.days\n",
        "\n",
        "# Remove the InvoiceDay column\n",
        "customer_data.drop(columns=['InvoiceDay'], inplace=True)\n"
      ],
      "metadata": {
        "id": "w91dU-pH7cyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data.head()"
      ],
      "metadata": {
        "id": "heJXyfen7iVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of transactions made by each customer\n",
        "total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "total_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\n",
        "\n",
        "# Calculate the total number of products purchased by each customer\n",
        "total_products_purchased = df.groupby('CustomerID')['Quantity'].sum().reset_index()\n",
        "total_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\n",
        "\n",
        "# Merge the new features into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, total_transactions, on='CustomerID')\n",
        "customer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "e7MKIks47kpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total spend by each customer\n",
        "df['Total_Spend'] = df['UnitPrice'] * df['Quantity']\n",
        "total_spend = df.groupby('CustomerID')['Total_Spend'].sum().reset_index()\n",
        "\n",
        "# Calculate the average transaction value for each customer\n",
        "average_transaction_value = total_spend.merge(total_transactions, on='CustomerID')\n",
        "average_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']\n",
        "\n",
        "# Merge the new features into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, total_spend, on='CustomerID')\n",
        "customer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "KNPdRxqb7nwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique products purchased by each customer\n",
        "unique_products_purchased = df.groupby('CustomerID')['StockCode'].nunique().reset_index()\n",
        "unique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)\n",
        "\n",
        "# Merge the new feature into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "1ttQtJlt7shn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract day of week and hour from InvoiceDate\n",
        "df['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek\n",
        "df['Hour'] = df['InvoiceDate'].dt.hour\n",
        "\n",
        "# Calculate the average number of days between consecutive purchases\n",
        "days_between_purchases = df.groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))\n",
        "average_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()\n",
        "average_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)\n",
        "\n",
        "# Find the favorite shopping day of the week\n",
        "favorite_shopping_day = df.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\n",
        "favorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]\n",
        "\n",
        "# Find the favorite shopping hour of the day\n",
        "favorite_shopping_hour = df.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\n",
        "favorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\n",
        "\n",
        "# Merge the new features into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')\n",
        "customer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\n",
        "customer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()\n"
      ],
      "metadata": {
        "id": "VOeQWYTd7vow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Country'].value_counts(normalize=True).head()"
      ],
      "metadata": {
        "id": "Hrte1Wzl71ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by CustomerID and Country to get the number of transactions per country for each customer\n",
        "customer_country = df.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\n",
        "\n",
        "# Get the country with the maximum number of transactions for each customer (in case a customer has transactions from multiple countries)\n",
        "customer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\n",
        "\n",
        "# Create a binary column indicating whether the customer is from the UK or not\n",
        "customer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\n",
        "\n",
        "# Merge this data with our customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "A3p6AtiF75B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display feature distribution\n",
        "customer_data['Is_UK'].value_counts()"
      ],
      "metadata": {
        "id": "lwQLBno479EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of transactions made by each customer\n",
        "total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "\n",
        "# Calculate the number of cancelled transactions for each customer\n",
        "cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\n",
        "cancellation_frequency = cancelled_transactions.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "cancellation_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)\n",
        "\n",
        "# Merge the Cancellation Frequency data into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, cancellation_frequency, on='CustomerID', how='left')\n",
        "\n",
        "# Replace NaN values with 0 (for customers who have not cancelled any transaction)\n",
        "customer_data['Cancellation_Frequency'].fillna(0, inplace=True)\n",
        "\n",
        "# Calculate the Cancellation Rate\n",
        "customer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / total_transactions['InvoiceNo']\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()\n"
      ],
      "metadata": {
        "id": "qhQ59ifd8ASq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month and year from InvoiceDate\n",
        "df['Year'] = df['InvoiceDate'].dt.year\n",
        "df['Month'] = df['InvoiceDate'].dt.month\n",
        "\n",
        "# Calculate monthly spending for each customer\n",
        "monthly_spending = df.groupby(['CustomerID', 'Year', 'Month'])['Total_Spend'].sum().reset_index()\n",
        "\n",
        "# Calculate Seasonal Buying Patterns: We are using monthly frequency as a proxy for seasonal buying patterns\n",
        "seasonal_buying_patterns = monthly_spending.groupby('CustomerID')['Total_Spend'].agg(['mean', 'std']).reset_index()\n",
        "seasonal_buying_patterns.rename(columns={'mean': 'Monthly_Spending_Mean', 'std': 'Monthly_Spending_Std'}, inplace=True)\n",
        "\n",
        "# Replace NaN values in Monthly_Spending_Std with 0, implying no variability for customers with single transaction month\n",
        "seasonal_buying_patterns['Monthly_Spending_Std'].fillna(0, inplace=True)\n",
        "\n",
        "# Calculate Trends in Spending\n",
        "# We are using the slope of the linear trend line fitted to the customer's spending over time as an indicator of spending trends\n",
        "def calculate_trend(spend_data):\n",
        "    # If there are more than one data points, we calculate the trend using linear regression\n",
        "    if len(spend_data) > 1:\n",
        "        x = np.arange(len(spend_data))\n",
        "        slope, _, _, _, _ = linregress(x, spend_data)\n",
        "        return slope\n",
        "    # If there is only one data point, no trend can be calculated, hence we return 0\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Apply the calculate_trend function to find the spending trend for each customer\n",
        "spending_trends = monthly_spending.groupby('CustomerID')['Total_Spend'].apply(calculate_trend).reset_index()\n",
        "spending_trends.rename(columns={'Total_Spend': 'Spending_Trend'}, inplace=True)\n",
        "\n",
        "# Merge the new features into the customer_data dataframe\n",
        "customer_data = pd.merge(customer_data, seasonal_buying_patterns, on='CustomerID')\n",
        "customer_data = pd.merge(customer_data, spending_trends, on='CustomerID')\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "eW9LOd1n8GBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the data type of 'CustomerID' to string as it is a unique identifier and not used in mathematical operations\n",
        "customer_data['CustomerID'] = customer_data['CustomerID'].astype(str)\n",
        "\n",
        "# Convert data types of columns to optimal types\n",
        "customer_data = customer_data.convert_dtypes()"
      ],
      "metadata": {
        "id": "1VfsGqIC8Obw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data.head(10)"
      ],
      "metadata": {
        "id": "bfIXoHtF8ROf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data.info()"
      ],
      "metadata": {
        "id": "mB3VTm1F8TkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the IsolationForest model with a contamination parameter of 0.05\n",
        "model = IsolationForest(contamination=0.05, random_state=0)\n",
        "\n",
        "# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\n",
        "customer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())\n",
        "\n",
        "# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n",
        "customer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]\n",
        "\n",
        "# Display the first few rows of the customer_data dataframe\n",
        "customer_data.head()"
      ],
      "metadata": {
        "id": "mRs83u9Z8V-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of inliers and outliers\n",
        "outlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plotting the percentage of inliers and outliers\n",
        "plt.figure(figsize=(12, 4))\n",
        "outlier_percentage.plot(kind='barh', color='#ff6200')\n",
        "\n",
        "# Adding the percentage labels on the bars\n",
        "for index, value in enumerate(outlier_percentage):\n",
        "    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n",
        "\n",
        "plt.title('Percentage of Inliers and Outliers')\n",
        "plt.xticks(ticks=np.arange(0, 115, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "plt.ylabel('Is Outlier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MTV0ianP8aI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the outliers for analysis\n",
        "outliers_data = customer_data[customer_data['Is_Outlier'] == 1]\n",
        "\n",
        "# Remove the outliers from the main dataset\n",
        "customer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]\n",
        "\n",
        "# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\n",
        "customer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n",
        "\n",
        "# Reset the index of the cleaned data\n",
        "customer_data_cleaned.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "SjIC4_-O8ep7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the number of rows in the cleaned customer dataset\n",
        "customer_data_cleaned.shape[0]"
      ],
      "metadata": {
        "id": "Cc6kpoRq8mzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset background style\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Calculate the correlation matrix excluding the 'CustomerID' column\n",
        "corr = customer_data_cleaned.drop(columns=['CustomerID']).corr()\n",
        "\n",
        "# Define a custom colormap\n",
        "colors = ['#ff6200', '#ffcaa8', 'white', '#ffcaa8', '#ff6200']\n",
        "my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n",
        "\n",
        "# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its\n",
        "# top-left to bottom-right diagonal)\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask, k=1)] = True\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n",
        "plt.title('Correlation Matrix', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rk3F_tO98pmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# List of columns that don't need to be scaled\n",
        "columns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n",
        "\n",
        "# List of columns that need to be scaled\n",
        "columns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)\n",
        "\n",
        "# Copy the cleaned dataset\n",
        "customer_data_scaled = customer_data_cleaned.copy()\n",
        "\n",
        "# Applying the scaler to the necessary columns in the dataset\n",
        "customer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n",
        "\n",
        "# Display the first few rows of the scaled data\n",
        "customer_data_scaled.head()"
      ],
      "metadata": {
        "id": "jNM7WSyI8tvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting CustomerID as the index column\n",
        "customer_data_scaled.set_index('CustomerID', inplace=True)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA().fit(customer_data_scaled)\n",
        "\n",
        "# Calculate the Cumulative Sum of the Explained Variance\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Set the optimal k value (based on our analysis, we can choose 6)\n",
        "optimal_k = 6\n",
        "\n",
        "# Set seaborn plot style\n",
        "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n",
        "\n",
        "# Plot the cumulative explained variance against the number of components\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Bar chart for the explained variance of each component\n",
        "barplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n",
        "                      y=explained_variance_ratio,\n",
        "                      color='#fcc36d',\n",
        "                      alpha=0.8)\n",
        "\n",
        "# Line plot for the cumulative explained variance\n",
        "lineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n",
        "                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n",
        "\n",
        "# Plot optimal k value line\n",
        "optimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Number of Components', fontsize=14)\n",
        "plt.ylabel('Explained Variance', fontsize=14)\n",
        "plt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n",
        "\n",
        "# Customize ticks and legend\n",
        "plt.xticks(range(0, len(cumulative_explained_variance)))\n",
        "plt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n",
        "           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n",
        "           loc=(0.62, 0.1),\n",
        "           frameon=True,\n",
        "           framealpha=1.0,\n",
        "           edgecolor='#ff6200')\n",
        "\n",
        "# Display the variance values for both graphs on the plots\n",
        "x_offset = -0.3\n",
        "y_offset = 0.01\n",
        "for i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n",
        "    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "    if i > 0:\n",
        "        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "plt.grid(axis='both')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5sFZEcIc8y7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a PCA object with 6 components\n",
        "pca = PCA(n_components=6)\n",
        "\n",
        "# Fitting and transforming the original data to the new PCA dataframe\n",
        "customer_data_pca = pca.fit_transform(customer_data_scaled)\n",
        "\n",
        "# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n",
        "customer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# Adding the CustomerID index back to the new PCA dataframe\n",
        "customer_data_pca.index = customer_data_scaled.index"
      ],
      "metadata": {
        "id": "67Nac7bn86yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the resulting dataframe based on the PCs\n",
        "customer_data_pca.head()"
      ],
      "metadata": {
        "id": "Z2N1XcS88-Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to highlight the top 3 absolute values in each column of a dataframe\n",
        "def highlight_top3(column):\n",
        "    top3 = column.abs().nlargest(3).index\n",
        "    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n",
        "\n",
        "# Create the PCA component DataFrame and apply the highlighting function\n",
        "pc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],\n",
        "                     index=customer_data_scaled.columns)\n",
        "\n",
        "pc_df.style.apply(highlight_top3, axis=0)"
      ],
      "metadata": {
        "id": "zdhBKGGc9AqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plot style, and background color\n",
        "sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n",
        "\n",
        "# Set the color palette for the plot\n",
        "sns.set_palette(['#ff6200'])\n",
        "\n",
        "# Instantiate the clustering model with the specified parameters\n",
        "km = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "\n",
        "# Create a figure and axis with the desired size\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\n",
        "visualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(customer_data_pca)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show();\n"
      ],
      "metadata": {
        "id": "zcfq1D5I9D9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n",
        "    \"\"\"\n",
        "    Perform Silhouette analysis for a range of k values and visualize the results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the size of the figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n",
        "    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n",
        "\n",
        "    # Assign the first plot to the first row and both columns\n",
        "    first_plot = plt.subplot(grid[0, :])\n",
        "\n",
        "    # First plot: Silhouette scores for different k values\n",
        "    sns.set_palette(['darkorange'])\n",
        "\n",
        "    silhouette_scores = []\n",
        "\n",
        "    # Iterate through the range of k values\n",
        "    for k in range(start_k, stop_k + 1):\n",
        "        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "        km.fit(df)\n",
        "        labels = km.predict(df)\n",
        "        score = silhouette_score(df, labels)\n",
        "        silhouette_scores.append(score)\n",
        "\n",
        "    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n",
        "\n",
        "    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n",
        "    plt.xticks(range(start_k, stop_k + 1))\n",
        "    plt.xlabel('Number of clusters (k)')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n",
        "\n",
        "    # Add the optimal k value text to the plot\n",
        "    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n",
        "    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom',\n",
        "             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n",
        "\n",
        "\n",
        "    # Second plot (subplot): Silhouette plots for each k value\n",
        "    colors = sns.color_palette(\"bright\")\n",
        "\n",
        "    for i in range(start_k, stop_k + 1):\n",
        "        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "        row_idx, col_idx = divmod(i - start_k, 2)\n",
        "\n",
        "        # Assign the plots to the second, third, and fourth rows\n",
        "        ax = plt.subplot(grid[row_idx + 1, col_idx])\n",
        "\n",
        "        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n",
        "        visualizer.fit(df)\n",
        "\n",
        "        # Add the Silhouette score text to the plot\n",
        "        score = silhouette_score(df, km.labels_)\n",
        "        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n",
        "                ha='right', transform=ax.transAxes, color='red')\n",
        "\n",
        "        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4dmVKwPz9J_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))"
      ],
      "metadata": {
        "id": "J527H_TM9RAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans clustering using the optimal k\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n",
        "kmeans.fit(customer_data_pca)\n",
        "\n",
        "# Get the frequency of each cluster\n",
        "cluster_frequencies = Counter(kmeans.labels_)\n",
        "\n",
        "# Create a mapping from old labels to new labels based on frequency\n",
        "label_mapping = {label: new_label for new_label, (label, _) in\n",
        "                 enumerate(cluster_frequencies.most_common())}\n",
        "\n",
        "# Reverse the mapping to assign labels as per your criteria\n",
        "label_mapping = {v: k for k, v in {2: 1, 1: 0, 0: 2}.items()}\n",
        "\n",
        "# Apply the mapping to get the new labels\n",
        "new_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n",
        "\n",
        "# Append the new cluster labels back to the original dataset\n",
        "customer_data_cleaned['cluster'] = new_labels\n",
        "\n",
        "# Append the new cluster labels to the PCA version of the dataset\n",
        "customer_data_pca['cluster'] = new_labels"
      ],
      "metadata": {
        "id": "UHroNZZr9U75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the original dataframe\n",
        "customer_data_cleaned.head()"
      ],
      "metadata": {
        "id": "8g1yH8T39b8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the color scheme for the clusters (RGB order)\n",
        "colors = ['#e8000b', '#1ac938', '#023eff']"
      ],
      "metadata": {
        "id": "beT9nJUf9e_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create separate data frames for each cluster\n",
        "cluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\n",
        "cluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\n",
        "cluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add data points for each cluster separately and specify the color\n",
        "fig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'],\n",
        "                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\n",
        "fig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'],\n",
        "                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\n",
        "fig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'],\n",
        "                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n",
        "\n",
        "# Set the title and layout details\n",
        "fig.update_layout(\n",
        "    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n",
        "    scene=dict(\n",
        "        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n",
        "        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n",
        "        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n",
        "    ),\n",
        "    width=900,\n",
        "    height=800\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "b4fHt5uz9mp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of customers in each cluster\n",
        "cluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n",
        "\n",
        "# Adding percentages on the bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Distribution of Customers Across Clusters', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('Percentage (%)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "egJViuVC9qu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute number of customers\n",
        "num_observations = len(customer_data_pca)\n",
        "\n",
        "# Separate the features and the cluster labels\n",
        "X = customer_data_pca.drop('cluster', axis=1)\n",
        "clusters = customer_data_pca['cluster']\n",
        "\n",
        "# Compute the metrics\n",
        "sil_score = silhouette_score(X, clusters)\n",
        "calinski_score = calinski_harabasz_score(X, clusters)\n",
        "davies_score = davies_bouldin_score(X, clusters)\n",
        "\n",
        "# Create a table to display the metrics and the number of observations\n",
        "table_data = [\n",
        "    [\"Number of Observations\", num_observations],\n",
        "    [\"Silhouette Score\", sil_score],\n",
        "    [\"Calinski Harabasz Score\", calinski_score],\n",
        "    [\"Davies Bouldin Score\", davies_score]\n",
        "]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))\n"
      ],
      "metadata": {
        "id": "3V8wNXxH9-cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting 'CustomerID' column as index and assigning it to a new dataframe\n",
        "df_customer = customer_data_cleaned.set_index('CustomerID')\n",
        "\n",
        "# Standardize the data (excluding the cluster column)\n",
        "scaler = StandardScaler()\n",
        "df_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n",
        "\n",
        "# Create a new dataframe with standardized values and add the cluster column back\n",
        "df_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\n",
        "df_customer_standardized['cluster'] = df_customer['cluster']\n",
        "\n",
        "# Calculate the centroids of each cluster\n",
        "cluster_centroids = df_customer_standardized.groupby('cluster').mean()\n",
        "\n",
        "# Function to create a radar chart\n",
        "def create_radar_chart(ax, angles, data, color, cluster):\n",
        "    # Plot the data and fill the area\n",
        "    ax.fill(angles, data, color=color, alpha=0.4)\n",
        "    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n",
        "\n",
        "    # Add a title\n",
        "    ax.set_title(f'Cluster {cluster}', size=20, color=color, y=1.1)\n",
        "\n",
        "# Set data\n",
        "labels=np.array(cluster_centroids.columns)\n",
        "num_vars = len(labels)\n",
        "\n",
        "# Compute angle of each axis\n",
        "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "\n",
        "# The plot is circular, so we need to \"complete the loop\" and append the start to the end\n",
        "labels = np.concatenate((labels, [labels[0]]))\n",
        "angles += angles[:1]\n",
        "\n",
        "# Initialize the figure\n",
        "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n",
        "\n",
        "# Create radar chart for each cluster\n",
        "for i, color in enumerate(colors):\n",
        "    data = cluster_centroids.loc[i].tolist()\n",
        "    data += data[:1]  # Complete the loop\n",
        "    create_radar_chart(ax[i], angles, data, color, i)\n",
        "\n",
        "# Add input data\n",
        "ax[0].set_xticks(angles[:-1])\n",
        "ax[0].set_xticklabels(labels[:-1])\n",
        "\n",
        "ax[1].set_xticks(angles[:-1])\n",
        "ax[1].set_xticklabels(labels[:-1])\n",
        "\n",
        "ax[2].set_xticks(angles[:-1])\n",
        "ax[2].set_xticklabels(labels[:-1])\n",
        "\n",
        "# Add a grid\n",
        "ax[0].grid(color='grey', linewidth=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eoOFo0Pt-FzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histograms for each feature segmented by the clusters\n",
        "features = customer_data_cleaned.columns[1:-1]\n",
        "clusters = customer_data_cleaned['cluster'].unique()\n",
        "clusters.sort()\n",
        "\n",
        "# Setting up the subplots\n",
        "n_rows = len(features)\n",
        "n_cols = len(clusters)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n",
        "\n",
        "# Plotting histograms\n",
        "for i, feature in enumerate(features):\n",
        "    for j, cluster in enumerate(clusters):\n",
        "        data = customer_data_cleaned[customer_data_cleaned['cluster'] == cluster][feature]\n",
        "        axes[i, j].hist(data, bins=20, color=colors[j], edgecolor='w', alpha=0.7)\n",
        "        axes[i, j].set_title(f'Cluster {cluster} - {feature}', fontsize=15)\n",
        "        axes[i, j].set_xlabel('')\n",
        "        axes[i, j].set_ylabel('')\n",
        "\n",
        "# Adjusting layout to prevent overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FibRsVrg-MJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract the CustomerIDs of the outliers and remove their transactions from the main dataframe\n",
        "outlier_customer_ids = outliers_data['CustomerID'].astype('float').unique()\n",
        "df_filtered = df[~df['CustomerID'].isin(outlier_customer_ids)]\n",
        "\n",
        "# Step 2: Ensure consistent data type for CustomerID across both dataframes before merging\n",
        "customer_data_cleaned['CustomerID'] = customer_data_cleaned['CustomerID'].astype('float')\n",
        "\n",
        "# Step 3: Merge the transaction data with the customer data to get the cluster information for each transaction\n",
        "merged_data = df_filtered.merge(customer_data_cleaned[['CustomerID', 'cluster']], on='CustomerID', how='inner')\n",
        "\n",
        "# Step 4: Identify the top 10 best-selling products in each cluster based on the total quantity sold\n",
        "best_selling_products = merged_data.groupby(['cluster', 'StockCode', 'Description'])['Quantity'].sum().reset_index()\n",
        "best_selling_products = best_selling_products.sort_values(by=['cluster', 'Quantity'], ascending=[True, False])\n",
        "top_products_per_cluster = best_selling_products.groupby('cluster').head(10)\n",
        "\n",
        "# Step 5: Create a record of products purchased by each customer in each cluster\n",
        "customer_purchases = merged_data.groupby(['CustomerID', 'cluster', 'StockCode'])['Quantity'].sum().reset_index()\n",
        "\n",
        "# Step 6: Generate recommendations for each customer in each cluster\n",
        "recommendations = []\n",
        "for cluster in top_products_per_cluster['cluster'].unique():\n",
        "    top_products = top_products_per_cluster[top_products_per_cluster['cluster'] == cluster]\n",
        "    customers_in_cluster = customer_data_cleaned[customer_data_cleaned['cluster'] == cluster]['CustomerID']\n",
        "\n",
        "    for customer in customers_in_cluster:\n",
        "        # Identify products already purchased by the customer\n",
        "        customer_purchased_products = customer_purchases[(customer_purchases['CustomerID'] == customer) &\n",
        "                                                         (customer_purchases['cluster'] == cluster)]['StockCode'].tolist()\n",
        "\n",
        "        # Find top 3 products in the best-selling list that the customer hasn't purchased yet\n",
        "        top_products_not_purchased = top_products[~top_products['StockCode'].isin(customer_purchased_products)]\n",
        "        top_3_products_not_purchased = top_products_not_purchased.head(3)\n",
        "\n",
        "        # Append the recommendations to the list\n",
        "        recommendations.append([customer, cluster] + top_3_products_not_purchased[['StockCode', 'Description']].values.flatten().tolist())\n",
        "\n",
        "# Step 7: Create a dataframe from the recommendations list and merge it with the original customer data\n",
        "recommendations_df = pd.DataFrame(recommendations, columns=['CustomerID', 'cluster', 'Rec1_StockCode', 'Rec1_Description', \\\n",
        "                                                 'Rec2_StockCode', 'Rec2_Description', 'Rec3_StockCode', 'Rec3_Description'])\n",
        "customer_data_with_recommendations = customer_data_cleaned.merge(recommendations_df, on=['CustomerID', 'cluster'], how='right')\n"
      ],
      "metadata": {
        "id": "G-GkKqMY-Rqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 10 random rows from the customer_data_with_recommendations dataframe\n",
        "customer_data_with_recommendations.set_index('CustomerID').iloc[:, -6:].sample(10, random_state=0)"
      ],
      "metadata": {
        "id": "spMu_fLO-b5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AIu_LCR0-gYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}